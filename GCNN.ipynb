{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5OAhzo75jTD"
      },
      "source": [
        "### This is a step by step walk through a Pytorch implementation of a gated convolutional neural network (GCNN) from Dauphin et al.'s 2017 paper. I use the wikitext-2 dataset.\n",
        "### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh9yqzmj5jTI"
      },
      "source": [
        "# Download and preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.2.4"
      ],
      "metadata": {
        "id": "FABdN-oAIaeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastai==1.0.61"
      ],
      "metadata": {
        "id": "0GgUfSM6GmH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddCJdvvh5jTN"
      },
      "outputs": [],
      "source": [
        "from fastai import *\n",
        "from fastai.text import * \n",
        "\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import time\n",
        "import importlib\n",
        "import seaborn as sns\n",
        "\n",
        "from GCNN_textfuncs import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA9A9JHB5jTO"
      },
      "outputs": [],
      "source": [
        "DATAPATH = Path('/data/GCNN/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wdt07L-5jTP"
      },
      "outputs": [],
      "source": [
        "sns.set() #set graph formatting to seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl3O9PTP5jTQ",
        "outputId": "f34ac0c6-9dab-4706-8598-a4feef513dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2018-12-10 20:33:38--  https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.236.53\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.236.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4070055 (3.9M) [application/x-tar]\n",
            "Saving to: '/data/wikitext-2.tgz.1'\n",
            "\n",
            "wikitext-2.tgz.1    100%[===================>]   3.88M   723KB/s    in 6.5s    \n",
            "\n",
            "2018-12-10 20:33:45 (610 KB/s) - '/data/wikitext-2.tgz.1' saved [4070055/4070055]\n",
            "\n",
            "--2018-12-10 20:33:46--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-12-10 20:33:46--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: '/data/GCNN/glove.6B.zip'\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M   993KB/s    in 14m 1s  \n",
            "\n",
            "2018-12-10 20:47:47 (1001 KB/s) - '/data/GCNN/glove.6B.zip' saved [862182613/862182613]\n",
            "\n",
            "Archive:  /data/GCNN/glove.6B.zip\n",
            "  inflating: /data/GCNN/glove.6B.50d.txt  \n",
            "  inflating: /data/GCNN/glove.6B.100d.txt  \n",
            "  inflating: /data/GCNN/glove.6B.200d.txt  \n",
            "  inflating: /data/GCNN/glove.6B.300d.txt  \n"
          ]
        }
      ],
      "source": [
        "#download wikitext-2 dataset and GloVe embeddings\n",
        "!wget https://s3.amazonaws.com/fast-ai-nlp/wikitext-2.tgz -P /data\n",
        "!tar xzf /data/wikitext-2.tgz -C /data\n",
        "!mv /data/wikitext-2/ /data/GCNN/\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip -P /data/GCNN/\n",
        "!unzip /data/GCNN/glove.6B.zip -d /data/GCNN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stfy-wmK5jTS"
      },
      "outputs": [],
      "source": [
        "#Do some preprocessing of the data:\n",
        "\n",
        "#put data into df's w/ columns for 'labels' and 'text'\n",
        "df_trn = pd.read_csv(DATAPATH/'train.csv',header=None,names=['text'])\n",
        "df_test = pd.read_csv(DATAPATH/'test.csv',header=None,names=['text'])\n",
        "df_trn['labels']=0\n",
        "df_test['labels']=0\n",
        "df_trn=df_trn[['labels','text']]\n",
        "df_test=df_test[['labels','text']]\n",
        "\n",
        "#split data into paragraphs, then remove paragraphs <10 or >300 words\n",
        "trn_paragraphs=[]\n",
        "for docnum in range(len(df_trn)):\n",
        "    trn_paragraphs.extend([x for x in df_trn.iloc[docnum].text.split('\\n')])\n",
        "trn_paragraphs.sort(key=len)\n",
        "trn_paragraphs=[par for par in trn_paragraphs if (len(par.split(' '))<300 and len(par.split(' '))>10)]#remove paragraphs >300 and <10 words\n",
        "trn_paragraphs=[par+'xxeos ' for par in trn_paragraphs] #add EOS token at end of each paragraph\n",
        "\n",
        "test_paragraphs=[]\n",
        "for docnum in range(len(df_test)):\n",
        "    test_paragraphs.extend([x for x in df_test.iloc[docnum].text.split('\\n')])\n",
        "test_paragraphs.sort(key=len)\n",
        "test_paragraphs=[par for par in test_paragraphs if (len(par.split(' '))<300 and len(par.split(' '))>10)]#remove paragraphs >300 and <10 words\n",
        "test_paragraphs=[par+'xxeos ' for par in test_paragraphs] #add EOS token at end of each paragraph\n",
        "\n",
        "#put data into csv's\n",
        "df_trn_par = pd.DataFrame({'text':trn_paragraphs})\n",
        "df_test_par = pd.DataFrame({'text':test_paragraphs})\n",
        "\n",
        "df_trn_par['labels']=0\n",
        "df_test_par['labels']=0\n",
        "df_trn_par=df_trn_par[['labels','text']]\n",
        "df_test_par=df_test_par[['labels','text']]\n",
        "\n",
        "df_trn_par.to_csv(DATAPATH/'train_proc_par2.csv', header=False, index=False)\n",
        "df_test_par.to_csv(DATAPATH/'test_proc_par2.csv', header=False, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ohvA0vU5jTT"
      },
      "source": [
        "# Create modeler class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h91PRbLL5jTV"
      },
      "outputs": [],
      "source": [
        "class modeler():\n",
        "    def __init__(self,trn_dl,val_dl,module,modelvals=None):\n",
        "        self.trn_dl, self.val_dl, self.module = trn_dl, val_dl, module\n",
        "        self.modelvals=modelvals\n",
        "        self.model=self.module.cuda()\n",
        "    def model_fit(self):\n",
        "        samp_n=self.modelvals['samp_n']#the number of iterations in an epoch\n",
        "        starttime=time.time()\n",
        "        train_loss_list=[]; val_loss_list=[]\n",
        "        for epoch in range(0, self.modelvals['epochs']):\n",
        "            pbar=0#progressbar\n",
        "            for batch_idx, (data, target) in enumerate(self.trn_dl):\n",
        "                \n",
        "                #GRAB MINIBATCH OF INPUTS AND TARGETS, SET OPTIMIZER\n",
        "                data = Variable(data)\n",
        "                pbar+=self.modelvals['bs'] #how many iterations have we done in the epoch so far\n",
        "                if self.modelvals['opttype']=='sgd':\n",
        "                    self.optimizer = optim.SGD(self.model.parameters(), lr=self.modelvals['lr'], \n",
        "                                       momentum=self.modelvals['mom'], weight_decay=self.modelvals['wd'],\n",
        "                                              nesterov=self.modelvals['nesterov'])\n",
        "                elif self.modelvals['opttype']=='adam':\n",
        "                    self.optimizer = optim.Adam(self.model.parameters(), lr=self.modelvals['lr'], \n",
        "                                        betas=(self.modelvals['mom'], 0.999))\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                #FORWARD PASS\n",
        "                output = self.model(data)\n",
        "                \n",
        "                #CALCULATE AND BACKPROP THE LOSS\n",
        "                loss= output.loss\n",
        "                loss.backward()\n",
        "                if self.modelvals['grad_clip']!=0: #gradient clipping\n",
        "                    torch.nn.utils.clip_grad_value_(self.model.parameters(), self.modelvals['grad_clip'])\n",
        "                    \n",
        "                #UPDATE THE WEIGHTS\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                #PRINT OUT TRAINING UPDATES\n",
        "                train_loss_list.append([epoch,pbar+epoch*samp_n,loss.data.item(),self.modelvals['lr']])\n",
        "                if batch_idx % 100 == 0:\n",
        "                    elapsed_time=time.time()-starttime\n",
        "                    train_update_format_string = 'Train Epoch: {}'\n",
        "                    train_update_format_string += '\\tTotal_its: {:.2f}M [{:.2f}M/{:.2f}M]'\n",
        "                    train_update_format_string += '\\tPercdone: {:.2f}'\n",
        "                    train_update_format_string += '\\tLoss: {:.4f}'\n",
        "                    train_update_format_string += '\\tTime: {:.2f}'\n",
        "                    train_update_format_string += '\\tLR: {:.4f}'\n",
        "                    train_update_string=train_update_format_string.format(\n",
        "                            epoch,\n",
        "                            (pbar + epoch * samp_n) / 1000000, pbar / 1000000, samp_n / 1000000,\n",
        "                            pbar / samp_n,\n",
        "                            loss.data.item(),\n",
        "                            elapsed_time / 60,\n",
        "                            self.modelvals['lr'])\n",
        "                    print(train_update_string)\n",
        "            final_train_loss=loss.data.item()\n",
        "            \n",
        "            #NOW TEST VALIDATION SET\n",
        "            val_loss=[]\n",
        "            self.model.eval() #important to set to eval mode for testing, so that eg batchnorm and dropout aren't used\n",
        "            for batch_idx, (data, target) in enumerate(self.val_dl):\n",
        "                data = Variable(data)\n",
        "                self.optimizer.zero_grad()\n",
        "                #ONLY NEED FORWARD PASS... NO BACKPROP\n",
        "                output = self.model(data)\n",
        "                loss= output.loss\n",
        "                output=output.output\n",
        "                val_loss.append(loss.data.item())\n",
        "            self.model.train() #set back to training mode\n",
        "            ave_val_loss=sum(val_loss) / len(val_loss)\n",
        "            val_update_string='Validation Loss: {:.4f}\\tPerp: {:.4f}'.format(\n",
        "                ave_val_loss,np.exp(ave_val_loss))\n",
        "            print(val_update_string)\n",
        "            val_loss_list.append([epoch,ave_val_loss, np.exp(ave_val_loss),elapsed_time/60])\n",
        "        self.modelvals['val_loss_list']=val_loss_list\n",
        "        self.modelvals['train_loss_list']=train_loss_list\n",
        "        print('The end! {:.2f} minutes'.format((time.time()-starttime)/60))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTH6rkrG5jTW"
      },
      "source": [
        "# Set hyperparameters and build embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMyzyOr5jTX"
      },
      "outputs": [],
      "source": [
        "#set hyperparameters\n",
        "bs=50 #batch-size\n",
        "emb_sz=300 #size of the embedding matrix\n",
        "nl=4 #number of layers\n",
        "nh=600 #number hidden units\n",
        "lr=1 #learning rate\n",
        "mom=.95 #momentum\n",
        "wd=5e-5 #weight-decay. Only has effect if opttype==sgd\n",
        "epochs=50\n",
        "nesterov=True #Nesterov momentum. only has effect if opttype==sgd\n",
        "grad_clip=0.07 #gradient clipping value. Set to 0 for no effect. See nn.utils.clip_grad_value_\n",
        "opttype='sgd' #adam, sgd\n",
        "k=4 #kernel_width\n",
        "downbot=20# in the bottleneck layers, how much to decrease channel depth?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3kRgwxl5jTY"
      },
      "outputs": [],
      "source": [
        "#Use fast.ai to create a TextLMDataBunch object. See http://docs.fast.ai/text.data.html#class-textlmdatabunch\n",
        "#This tokenizes and numericalizes the data\n",
        "data_lm = TextLMDataBunch.from_csv(path=DATAPATH, csv_name='train_proc_par2.csv', test='test_proc_par2.csv')\n",
        "itos=data_lm.train_ds.vocab.itos# the vocab\n",
        "vs=len(itos)# vs is the length of the vocab\n",
        "\n",
        "#Grab the numericalized data from the TextLMDataBunch dataset, then construct new custom dataset using LMDataset_GCNN\n",
        "trn_tokens=[data_lm.train_ds[i][0].data for i in range(len(data_lm.train_ds))]\n",
        "traindataset=LMDataset_GCNN(trn_tokens)\n",
        "valid_tokens=[data_lm.valid_ds[i][0].data for i in range(len(data_lm.valid_ds))]\n",
        "validdataset=LMDataset_GCNN(valid_tokens)\n",
        "\n",
        "#Create data loaders for training and validation sets\n",
        "trn_samp=SortishSampler_GCNN(data_length=len(traindataset),key=lambda x:len(traindataset[x][0]), bs=bs)\n",
        "val_samp=SortSampler_GCNN(validdataset,key=lambda x:len(validdataset[x][0]))\n",
        "train_loader = data_utils.DataLoader(traindataset, batch_size=bs, collate_fn=pad_collate_GCNN, sampler=trn_samp)\n",
        "val_loader = data_utils.DataLoader(validdataset, batch_size=bs, collate_fn=pad_collate_GCNN,sampler=val_samp)\n",
        "samp_n=len(traindataset)\n",
        "val_samp_n=len(validdataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raxdVjET5jTY"
      },
      "outputs": [],
      "source": [
        "#put hyperparameters into a dictionary\n",
        "def get_modelvals():\n",
        "    modelvals=dict((name,eval(name)) for name in [\n",
        "        'lr','mom','wd','opttype','epochs','samp_n','val_samp_n',\n",
        "        'bs','emb_sz','vs', 'nh', 'nl','DATAPATH','nesterov','grad_clip',\n",
        "        'k','downbot'] )\n",
        "    return modelvals\n",
        "\n",
        "modelvals=get_modelvals()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5W8gvNM5jTZ"
      },
      "outputs": [],
      "source": [
        "#grab GloVe embeddings:\n",
        "#create vocab itos2 from downloaded glove file\n",
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = []\n",
        "with open('/data/GCNN/glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vectors.append(line[1:])\n",
        "itos2=words\n",
        "\n",
        "#grab the glove embeddings we need, based on the words in our vocab\n",
        "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)}) #default -1 means its not in glove's itos2\n",
        "row_m = vectors[-1] #this is default vector... for <unk>\n",
        "new_w = np.zeros((vs, emb_sz), dtype=np.float32)#initialize new weights to zeros of size (vocab_size,embedding size) e.g. (60002,300)... we're creating an embedding matrix \n",
        "for i,w in enumerate(itos): #for index,word in our itos dict, get r index of the word in word2vec's dict. r will be -1 if it doesn't exist in word2vec's dict\n",
        "    r = stoi2[w]#r index of the word in word2vec's dict\n",
        "    new_w[i] = vectors[r] if r>=0 else row_m #for our new embedding matrix, set the embedding at the index from our dict equal to the embedding from index r from word2vec's dict\n",
        "np.save(DATAPATH/'emb_wgts300_proc_par2.npy', new_w) #save the embedding weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5p3SEAl5jTZ"
      },
      "source": [
        "# Run GCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CEpWmYI5jTa"
      },
      "outputs": [],
      "source": [
        "class GLUblock(nn.Module):\n",
        "    def __init__(self, k, in_c, out_c, downbot):\n",
        "        super().__init__()\n",
        "        #only need to change shape of the residual if num_channels changes (i.e. in_c != out_c)\n",
        "        #[bs,in_c,seq_length]->conv(1,in_c,out_c)->[bs,out_c,seq_length]\n",
        "        if in_c == out_c:\n",
        "            self.use_proj=0\n",
        "        else:\n",
        "            self.use_proj=1\n",
        "        self.convresid=nn.utils.weight_norm(nn.Conv2d(in_c, out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "        \n",
        "        self.leftpad = nn.ConstantPad2d((0,0,k-1,0),0)#(paddingLeft, paddingRight, paddingTop, paddingBottom)\n",
        "\n",
        "        #[bs,in_c,seq_length+(k-1)]->conv(1,in_c,in_c/downbot)->[bs,in_c/downbot,seq_length+(k-1)]\n",
        "        self.convx1a = nn.utils.weight_norm(nn.Conv2d(in_c, int(in_c/downbot), kernel_size=(1,1)),name='weight',dim=0)\n",
        "        self.convx2a = nn.utils.weight_norm(nn.Conv2d(in_c, int(in_c/downbot), kernel_size=(1,1)),name='weight',dim=0)\n",
        "        #[bs,in_c/downbot,seq_length+(k-1)]->conv(k,in_c/downbot,in_c/downbot)->[bs,in_c/downbot,seq_length]\n",
        "        self.convx1b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), int(in_c/downbot), kernel_size=(k,1)),name='weight',dim=0)\n",
        "        self.convx2b = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), int(in_c/downbot), kernel_size=(k,1)),name='weight',dim=0)\n",
        "        #[bs,in_c/downbot,seq_length]->conv(1,in_c/downbot,out_c)->[bs,out_c,seq_length]\n",
        "        self.convx1c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "        self.convx2c = nn.utils.weight_norm(nn.Conv2d(int(in_c/downbot), out_c, kernel_size=(1,1)),name='weight',dim=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        if self.use_proj==1:# if in_c != out_c, need to change size of residual\n",
        "            residual=self.convresid(residual)\n",
        "        x=self.leftpad(x) # [bs,in_c,seq_length+(k-1),1]\n",
        "        x1 = self.convx1c(self.convx1b(self.convx1a(x))) # [bs,out_c,seq_length,1]\n",
        "        x2 = self.convx2c(self.convx2b(self.convx2a(x))) # [bs,out_c,seq_length,1]\n",
        "        x2 = torch.sigmoid(x2)\n",
        "        x=torch.mul(x1,x2) # [bs,out_c,seq_length,1]\n",
        "        return x+residual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oqlxmtm5jTa"
      },
      "outputs": [],
      "source": [
        "class GCNNmodel(nn.Module):\n",
        "    def __init__(self, vs, emb_sz, k, nh, nl,downbot):\n",
        "    #def __init__(self, vs, emb_sz, k, nh, nl,dw,cutoffs):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(vs, emb_sz)\n",
        "        \n",
        "        self.inlayer=GLUblock(k,emb_sz,nh,downbot)\n",
        "        self.GLUlayers=self.make_GLU_layers(k,nh,nl,downbot)\n",
        "        self.out=nn.AdaptiveLogSoftmaxWithLoss(nh, vs, cutoffs=[round(vs/25),round(vs/5)],div_value=4)\n",
        "\n",
        "    def make_GLU_layers(self, k, nh, nl, downbot):\n",
        "        layers = [GLUblock(k, nh, nh, downbot) for i in range(nl)]\n",
        "        return nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        target=x[1:,:]\n",
        "        target=target.contiguous().view(target.size()[0]*target.size()[1])#[seq_length*bs,out_c]\n",
        "        x=x[:-1,:]\n",
        "        \n",
        "        #first block\n",
        "        x = self.embed(torch.t(x)) # x -> [seq_length,bs] -> [bs,seq_length] -> [bs,seq_length,emb_sz] ... i.e. transpose 1st\n",
        "        x=torch.transpose(x, 1, 2) #[bs,emb_sz,seq_length]    \n",
        "        x = x.unsqueeze(3)  # [bs,emb_sz,seq_length,1]\n",
        "        x=self.inlayer(x) #[bs,nh,seq_length,1]\n",
        "             \n",
        "        #residual GLU blocks\n",
        "        x=self.GLUlayers(x) # [bs,nh,seq_length,1]\n",
        "        \n",
        "        #out\n",
        "        x=torch.squeeze(x,3) #[bs,out_c,seq_length]\n",
        "        x=torch.transpose(x, 1, 2) #[bs,seq_length,out_c]\n",
        "        x=torch.transpose(x, 0, 1) #[seq_length,bs,out_c]\n",
        "        x=x.contiguous().view(-1,x.size()[2])#[seq_length*bs,out_c]\n",
        "        outta=self.out(x,target)\n",
        "        \n",
        "        return    outta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XoYkv4D5jTb",
        "outputId": "84d15512-9a0f-4998-f03c-6c6d2456bef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCNNmodel(\n",
            "  (embed): Embedding(25005, 300)\n",
            "  (inlayer): GLUblock(\n",
            "    (convresid): Conv2d(300, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "    (convx1a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (convx2a): Conv2d(300, 15, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (convx1b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
            "    (convx2b): Conv2d(15, 15, kernel_size=(4, 1), stride=(1, 1))\n",
            "    (convx1c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (convx2c): Conv2d(15, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (GLUlayers): Sequential(\n",
            "    (0): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (3): GLUblock(\n",
            "      (convresid): Conv2d(600, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (leftpad): ConstantPad2d(padding=(0, 0, 3, 0), value=0)\n",
            "      (convx1a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2a): Conv2d(600, 30, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx1b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx2b): Conv2d(30, 30, kernel_size=(4, 1), stride=(1, 1))\n",
            "      (convx1c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (convx2c): Conv2d(30, 600, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (out): AdaptiveLogSoftmaxWithLoss(\n",
            "    (head): Linear(in_features=600, out_features=1002, bias=False)\n",
            "    (tail): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Linear(in_features=600, out_features=150, bias=False)\n",
            "        (1): Linear(in_features=150, out_features=4001, bias=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Linear(in_features=600, out_features=37, bias=False)\n",
            "        (1): Linear(in_features=37, out_features=20004, bias=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#create GCNN \n",
        "GCNNnet=modeler(train_loader,val_loader,\n",
        "                           GCNNmodel(vs, emb_sz, k, nh, nl, downbot),modelvals)\n",
        "print(GCNNnet.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL0LgSiO5jTb"
      },
      "outputs": [],
      "source": [
        "#load the glove-vectors into the model\n",
        "new_w=np.load(DATAPATH/'emb_wgts300_proc_par2.npy') #load embedding weights\n",
        "GCNNnet.model.embed.weight.data=torch.FloatTensor(new_w).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeUnydin5jTc",
        "outputId": "af658c06-a1b5-475b-c533-cb54e2f7d0d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0\tTotal_its: 0.00M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 8.6694\tTime: 0.02\tLR: 1.0000\n",
            "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 84.8770\tTime: 0.41\tLR: 1.0000\n",
            "Train Epoch: 0\tTotal_its: 0.01M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 58.1936\tTime: 0.75\tLR: 1.0000\n",
            "Validation Loss: 19.4324\tPerp: 275028850.7105\n",
            "Train Epoch: 1\tTotal_its: 0.01M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 17.9454\tTime: 1.33\tLR: 1.0000\n",
            "Train Epoch: 1\tTotal_its: 0.02M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 10.2991\tTime: 1.72\tLR: 1.0000\n",
            "Train Epoch: 1\tTotal_its: 0.02M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.0886\tTime: 2.28\tLR: 1.0000\n",
            "Validation Loss: 9.0299\tPerp: 8348.8260\n",
            "Train Epoch: 2\tTotal_its: 0.03M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 8.2724\tTime: 2.79\tLR: 1.0000\n",
            "Train Epoch: 2\tTotal_its: 0.03M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 7.9598\tTime: 3.17\tLR: 1.0000\n",
            "Train Epoch: 2\tTotal_its: 0.04M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.8604\tTime: 3.56\tLR: 1.0000\n",
            "Validation Loss: 6.1673\tPerp: 476.9110\n",
            "Train Epoch: 3\tTotal_its: 0.04M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 5.6165\tTime: 4.14\tLR: 1.0000\n",
            "Train Epoch: 3\tTotal_its: 0.05M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 6.3661\tTime: 4.52\tLR: 1.0000\n",
            "Train Epoch: 3\tTotal_its: 0.05M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.8590\tTime: 4.86\tLR: 1.0000\n",
            "Validation Loss: 5.3866\tPerp: 218.4700\n",
            "Train Epoch: 4\tTotal_its: 0.06M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 5.0374\tTime: 5.34\tLR: 1.0000\n",
            "Train Epoch: 4\tTotal_its: 0.06M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.2268\tTime: 5.69\tLR: 1.0000\n",
            "Train Epoch: 4\tTotal_its: 0.07M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.2056\tTime: 6.17\tLR: 1.0000\n",
            "Validation Loss: 5.2598\tPerp: 192.4380\n",
            "Train Epoch: 5\tTotal_its: 0.07M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.6302\tTime: 6.78\tLR: 1.0000\n",
            "Train Epoch: 5\tTotal_its: 0.07M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.5237\tTime: 7.18\tLR: 1.0000\n",
            "Train Epoch: 5\tTotal_its: 0.08M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.6871\tTime: 7.55\tLR: 1.0000\n",
            "Validation Loss: 5.2052\tPerp: 182.2230\n",
            "Train Epoch: 6\tTotal_its: 0.08M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.8990\tTime: 8.09\tLR: 1.0000\n",
            "Train Epoch: 6\tTotal_its: 0.09M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.1158\tTime: 8.46\tLR: 1.0000\n",
            "Train Epoch: 6\tTotal_its: 0.09M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.6805\tTime: 8.94\tLR: 1.0000\n",
            "Validation Loss: 5.0845\tPerp: 161.4960\n",
            "Train Epoch: 7\tTotal_its: 0.10M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4816\tTime: 9.48\tLR: 1.0000\n",
            "Train Epoch: 7\tTotal_its: 0.10M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.0224\tTime: 9.89\tLR: 1.0000\n",
            "Train Epoch: 7\tTotal_its: 0.11M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.6455\tTime: 10.37\tLR: 1.0000\n",
            "Validation Loss: 5.0547\tPerp: 156.7502\n",
            "Train Epoch: 8\tTotal_its: 0.11M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.7264\tTime: 10.94\tLR: 1.0000\n",
            "Train Epoch: 8\tTotal_its: 0.12M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.2673\tTime: 11.34\tLR: 1.0000\n",
            "Train Epoch: 8\tTotal_its: 0.12M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.0858\tTime: 11.72\tLR: 1.0000\n",
            "Validation Loss: 5.2619\tPerp: 192.8555\n",
            "Train Epoch: 9\tTotal_its: 0.13M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.8520\tTime: 12.27\tLR: 1.0000\n",
            "Train Epoch: 9\tTotal_its: 0.13M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.4744\tTime: 12.63\tLR: 1.0000\n",
            "Train Epoch: 9\tTotal_its: 0.14M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.0996\tTime: 13.03\tLR: 1.0000\n",
            "Validation Loss: 4.9501\tPerp: 141.1929\n",
            "Train Epoch: 10\tTotal_its: 0.14M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.6203\tTime: 13.56\tLR: 1.0000\n",
            "Train Epoch: 10\tTotal_its: 0.14M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.9520\tTime: 13.95\tLR: 1.0000\n",
            "Train Epoch: 10\tTotal_its: 0.15M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.3717\tTime: 14.44\tLR: 1.0000\n",
            "Validation Loss: 4.8968\tPerp: 133.8666\n",
            "Train Epoch: 11\tTotal_its: 0.15M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.5133\tTime: 15.01\tLR: 1.0000\n",
            "Train Epoch: 11\tTotal_its: 0.16M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.9189\tTime: 15.43\tLR: 1.0000\n",
            "Train Epoch: 11\tTotal_its: 0.16M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.7814\tTime: 15.93\tLR: 1.0000\n",
            "Validation Loss: 4.8279\tPerp: 124.9533\n",
            "Train Epoch: 12\tTotal_its: 0.17M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4316\tTime: 16.46\tLR: 1.0000\n",
            "Train Epoch: 12\tTotal_its: 0.17M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.1590\tTime: 16.84\tLR: 1.0000\n",
            "Train Epoch: 12\tTotal_its: 0.18M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.2958\tTime: 17.21\tLR: 1.0000\n",
            "Validation Loss: 4.8253\tPerp: 124.6268\n",
            "Train Epoch: 13\tTotal_its: 0.18M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4220\tTime: 17.69\tLR: 1.0000\n",
            "Train Epoch: 13\tTotal_its: 0.19M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.2309\tTime: 18.04\tLR: 1.0000\n",
            "Train Epoch: 13\tTotal_its: 0.19M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.4893\tTime: 18.46\tLR: 1.0000\n",
            "Validation Loss: 4.7702\tPerp: 117.9387\n",
            "Train Epoch: 14\tTotal_its: 0.20M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3996\tTime: 19.03\tLR: 1.0000\n",
            "Train Epoch: 14\tTotal_its: 0.20M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0950\tTime: 19.47\tLR: 1.0000\n",
            "Train Epoch: 14\tTotal_its: 0.21M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.0739\tTime: 19.97\tLR: 1.0000\n",
            "Validation Loss: 4.7196\tPerp: 112.1190\n",
            "Train Epoch: 15\tTotal_its: 0.21M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3948\tTime: 20.49\tLR: 1.0000\n",
            "Train Epoch: 15\tTotal_its: 0.21M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.1700\tTime: 20.85\tLR: 1.0000\n",
            "Train Epoch: 15\tTotal_its: 0.22M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.6870\tTime: 21.24\tLR: 1.0000\n",
            "Validation Loss: 4.8769\tPerp: 131.2285\n",
            "Train Epoch: 16\tTotal_its: 0.22M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3590\tTime: 21.81\tLR: 1.0000\n",
            "Train Epoch: 16\tTotal_its: 0.23M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.4935\tTime: 22.27\tLR: 1.0000\n",
            "Train Epoch: 16\tTotal_its: 0.23M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.1645\tTime: 22.79\tLR: 1.0000\n",
            "Validation Loss: 4.6689\tPerp: 106.5857\n",
            "Train Epoch: 17\tTotal_its: 0.24M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3010\tTime: 23.31\tLR: 1.0000\n",
            "Train Epoch: 17\tTotal_its: 0.24M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 6.0338\tTime: 23.71\tLR: 1.0000\n",
            "Train Epoch: 17\tTotal_its: 0.25M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.3397\tTime: 24.06\tLR: 1.0000\n",
            "Validation Loss: 4.9044\tPerp: 134.8758\n",
            "Train Epoch: 18\tTotal_its: 0.25M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.6104\tTime: 24.57\tLR: 1.0000\n",
            "Train Epoch: 18\tTotal_its: 0.26M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.8994\tTime: 24.97\tLR: 1.0000\n",
            "Train Epoch: 18\tTotal_its: 0.26M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.1977\tTime: 25.48\tLR: 1.0000\n",
            "Validation Loss: 4.7991\tPerp: 121.3997\n",
            "Train Epoch: 19\tTotal_its: 0.26M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.4774\tTime: 26.02\tLR: 1.0000\n",
            "Train Epoch: 19\tTotal_its: 0.27M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.6319\tTime: 26.42\tLR: 1.0000\n",
            "Train Epoch: 19\tTotal_its: 0.27M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.8841\tTime: 26.92\tLR: 1.0000\n",
            "Validation Loss: 4.7361\tPerp: 113.9925\n",
            "Train Epoch: 20\tTotal_its: 0.28M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3513\tTime: 27.49\tLR: 1.0000\n",
            "Train Epoch: 20\tTotal_its: 0.28M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.5869\tTime: 27.95\tLR: 1.0000\n",
            "Train Epoch: 20\tTotal_its: 0.29M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.5135\tTime: 28.39\tLR: 1.0000\n",
            "Validation Loss: 4.7089\tPerp: 110.9319\n",
            "Train Epoch: 21\tTotal_its: 0.29M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3648\tTime: 28.96\tLR: 1.0000\n",
            "Train Epoch: 21\tTotal_its: 0.30M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.7563\tTime: 29.32\tLR: 1.0000\n",
            "Train Epoch: 21\tTotal_its: 0.30M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.9822\tTime: 29.68\tLR: 1.0000\n",
            "Validation Loss: 4.9916\tPerp: 147.1683\n",
            "Train Epoch: 22\tTotal_its: 0.31M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.5618\tTime: 30.22\tLR: 1.0000\n",
            "Train Epoch: 22\tTotal_its: 0.31M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.0024\tTime: 30.62\tLR: 1.0000\n",
            "Train Epoch: 22\tTotal_its: 0.32M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.1904\tTime: 31.03\tLR: 1.0000\n",
            "Validation Loss: 4.6293\tPerp: 102.4428\n",
            "Train Epoch: 23\tTotal_its: 0.32M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3365\tTime: 31.53\tLR: 1.0000\n",
            "Train Epoch: 23\tTotal_its: 0.33M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 5.0842\tTime: 31.85\tLR: 1.0000\n",
            "Train Epoch: 23\tTotal_its: 0.33M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.8928\tTime: 32.29\tLR: 1.0000\n",
            "Validation Loss: 4.7521\tPerp: 115.8234\n",
            "Train Epoch: 24\tTotal_its: 0.33M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.2743\tTime: 32.88\tLR: 1.0000\n",
            "Train Epoch: 24\tTotal_its: 0.34M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.9039\tTime: 33.20\tLR: 1.0000\n",
            "Train Epoch: 24\tTotal_its: 0.34M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.9656\tTime: 33.56\tLR: 1.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.6390\tPerp: 103.4425\n",
            "Train Epoch: 25\tTotal_its: 0.35M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3723\tTime: 34.09\tLR: 1.0000\n",
            "Train Epoch: 25\tTotal_its: 0.35M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.7923\tTime: 34.43\tLR: 1.0000\n",
            "Train Epoch: 25\tTotal_its: 0.36M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.5208\tTime: 34.81\tLR: 1.0000\n",
            "Validation Loss: 4.6359\tPerp: 103.1180\n",
            "Train Epoch: 26\tTotal_its: 0.36M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3438\tTime: 35.38\tLR: 1.0000\n",
            "Train Epoch: 26\tTotal_its: 0.37M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0997\tTime: 35.74\tLR: 1.0000\n",
            "Train Epoch: 26\tTotal_its: 0.37M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.1723\tTime: 36.23\tLR: 1.0000\n",
            "Validation Loss: 4.5227\tPerp: 92.0815\n",
            "Train Epoch: 27\tTotal_its: 0.38M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1727\tTime: 36.76\tLR: 1.0000\n",
            "Train Epoch: 27\tTotal_its: 0.38M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.7655\tTime: 37.13\tLR: 1.0000\n",
            "Train Epoch: 27\tTotal_its: 0.39M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.1541\tTime: 37.53\tLR: 1.0000\n",
            "Validation Loss: 4.5349\tPerp: 93.2151\n",
            "Train Epoch: 28\tTotal_its: 0.39M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.2471\tTime: 38.11\tLR: 1.0000\n",
            "Train Epoch: 28\tTotal_its: 0.40M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.0073\tTime: 38.52\tLR: 1.0000\n",
            "Train Epoch: 28\tTotal_its: 0.40M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.1431\tTime: 38.92\tLR: 1.0000\n",
            "Validation Loss: 4.5124\tPerp: 91.1428\n",
            "Train Epoch: 29\tTotal_its: 0.40M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0575\tTime: 39.46\tLR: 1.0000\n",
            "Train Epoch: 29\tTotal_its: 0.41M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.9370\tTime: 39.82\tLR: 1.0000\n",
            "Train Epoch: 29\tTotal_its: 0.41M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.6131\tTime: 40.19\tLR: 1.0000\n",
            "Validation Loss: 4.5202\tPerp: 91.8518\n",
            "Train Epoch: 30\tTotal_its: 0.42M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.2422\tTime: 40.72\tLR: 1.0000\n",
            "Train Epoch: 30\tTotal_its: 0.42M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.8149\tTime: 41.09\tLR: 1.0000\n",
            "Train Epoch: 30\tTotal_its: 0.43M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.2618\tTime: 41.50\tLR: 1.0000\n",
            "Validation Loss: 4.5992\tPerp: 99.4012\n",
            "Train Epoch: 31\tTotal_its: 0.43M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.2899\tTime: 42.06\tLR: 1.0000\n",
            "Train Epoch: 31\tTotal_its: 0.44M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.3326\tTime: 42.40\tLR: 1.0000\n",
            "Train Epoch: 31\tTotal_its: 0.44M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.8255\tTime: 42.79\tLR: 1.0000\n",
            "Validation Loss: 4.5738\tPerp: 96.9090\n",
            "Train Epoch: 32\tTotal_its: 0.45M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.3386\tTime: 43.32\tLR: 1.0000\n",
            "Train Epoch: 32\tTotal_its: 0.45M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.8841\tTime: 43.70\tLR: 1.0000\n",
            "Train Epoch: 32\tTotal_its: 0.46M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.9975\tTime: 44.21\tLR: 1.0000\n",
            "Validation Loss: 4.4784\tPerp: 88.0954\n",
            "Train Epoch: 33\tTotal_its: 0.46M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1303\tTime: 44.74\tLR: 1.0000\n",
            "Train Epoch: 33\tTotal_its: 0.46M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.8749\tTime: 45.09\tLR: 1.0000\n",
            "Train Epoch: 33\tTotal_its: 0.47M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.2743\tTime: 45.46\tLR: 1.0000\n",
            "Validation Loss: 4.4749\tPerp: 87.7841\n",
            "Train Epoch: 34\tTotal_its: 0.47M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1614\tTime: 46.05\tLR: 1.0000\n",
            "Train Epoch: 34\tTotal_its: 0.48M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.7210\tTime: 46.42\tLR: 1.0000\n",
            "Train Epoch: 34\tTotal_its: 0.48M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.7870\tTime: 46.77\tLR: 1.0000\n",
            "Validation Loss: 4.4799\tPerp: 88.2222\n",
            "Train Epoch: 35\tTotal_its: 0.49M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1572\tTime: 47.27\tLR: 1.0000\n",
            "Train Epoch: 35\tTotal_its: 0.49M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.5153\tTime: 47.64\tLR: 1.0000\n",
            "Train Epoch: 35\tTotal_its: 0.50M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.9522\tTime: 48.13\tLR: 1.0000\n",
            "Validation Loss: 4.5201\tPerp: 91.8445\n",
            "Train Epoch: 36\tTotal_its: 0.50M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1842\tTime: 48.70\tLR: 1.0000\n",
            "Train Epoch: 36\tTotal_its: 0.51M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.8094\tTime: 49.06\tLR: 1.0000\n",
            "Train Epoch: 36\tTotal_its: 0.51M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.8716\tTime: 49.42\tLR: 1.0000\n",
            "Validation Loss: 4.6240\tPerp: 101.8970\n",
            "Train Epoch: 37\tTotal_its: 0.52M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1961\tTime: 49.94\tLR: 1.0000\n",
            "Train Epoch: 37\tTotal_its: 0.52M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.8111\tTime: 50.37\tLR: 1.0000\n",
            "Train Epoch: 37\tTotal_its: 0.53M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.3497\tTime: 50.88\tLR: 1.0000\n",
            "Validation Loss: 5.3898\tPerp: 219.1675\n",
            "Train Epoch: 38\tTotal_its: 0.53M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.7917\tTime: 51.42\tLR: 1.0000\n",
            "Train Epoch: 38\tTotal_its: 0.53M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.6689\tTime: 51.84\tLR: 1.0000\n",
            "Train Epoch: 38\tTotal_its: 0.54M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.0009\tTime: 52.32\tLR: 1.0000\n",
            "Validation Loss: 4.4403\tPerp: 84.7964\n",
            "Train Epoch: 39\tTotal_its: 0.54M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0633\tTime: 52.86\tLR: 1.0000\n",
            "Train Epoch: 39\tTotal_its: 0.55M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.6915\tTime: 53.23\tLR: 1.0000\n",
            "Train Epoch: 39\tTotal_its: 0.55M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.4349\tTime: 53.65\tLR: 1.0000\n",
            "Validation Loss: 4.5088\tPerp: 90.8145\n",
            "Train Epoch: 40\tTotal_its: 0.56M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1189\tTime: 54.19\tLR: 1.0000\n",
            "Train Epoch: 40\tTotal_its: 0.56M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.6307\tTime: 54.56\tLR: 1.0000\n",
            "Train Epoch: 40\tTotal_its: 0.57M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.1683\tTime: 54.91\tLR: 1.0000\n",
            "Validation Loss: 4.4043\tPerp: 81.8024\n",
            "Train Epoch: 41\tTotal_its: 0.57M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0045\tTime: 55.38\tLR: 1.0000\n",
            "Train Epoch: 41\tTotal_its: 0.58M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.2796\tTime: 55.78\tLR: 1.0000\n",
            "Train Epoch: 41\tTotal_its: 0.58M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.7275\tTime: 56.28\tLR: 1.0000\n",
            "Validation Loss: 4.4137\tPerp: 82.5730\n",
            "Train Epoch: 42\tTotal_its: 0.58M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0356\tTime: 56.84\tLR: 1.0000\n",
            "Train Epoch: 42\tTotal_its: 0.59M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 4.4527\tTime: 57.25\tLR: 1.0000\n",
            "Train Epoch: 42\tTotal_its: 0.59M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.5311\tTime: 57.60\tLR: 1.0000\n",
            "Validation Loss: 4.4988\tPerp: 89.9090\n",
            "Train Epoch: 43\tTotal_its: 0.60M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.1005\tTime: 58.06\tLR: 1.0000\n",
            "Train Epoch: 43\tTotal_its: 0.60M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.1576\tTime: 58.48\tLR: 1.0000\n",
            "Train Epoch: 43\tTotal_its: 0.61M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 2.2688\tTime: 58.95\tLR: 1.0000\n",
            "Validation Loss: 4.4388\tPerp: 84.6761\n",
            "Train Epoch: 44\tTotal_its: 0.61M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0453\tTime: 59.52\tLR: 1.0000\n",
            "Train Epoch: 44\tTotal_its: 0.62M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.3627\tTime: 59.96\tLR: 1.0000\n",
            "Train Epoch: 44\tTotal_its: 0.62M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.8955\tTime: 60.46\tLR: 1.0000\n",
            "Validation Loss: 4.4294\tPerp: 83.8801\n",
            "Train Epoch: 45\tTotal_its: 0.63M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0813\tTime: 60.96\tLR: 1.0000\n",
            "Train Epoch: 45\tTotal_its: 0.63M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 2.2595\tTime: 61.32\tLR: 1.0000\n",
            "Train Epoch: 45\tTotal_its: 0.64M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.1504\tTime: 61.80\tLR: 1.0000\n",
            "Validation Loss: 4.4149\tPerp: 82.6749\n",
            "Train Epoch: 46\tTotal_its: 0.64M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 3.9627\tTime: 62.37\tLR: 1.0000\n",
            "Train Epoch: 46\tTotal_its: 0.65M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.0087\tTime: 62.82\tLR: 1.0000\n",
            "Train Epoch: 46\tTotal_its: 0.65M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.3756\tTime: 63.30\tLR: 1.0000\n",
            "Validation Loss: 4.4361\tPerp: 84.4412\n",
            "Train Epoch: 47\tTotal_its: 0.65M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0805\tTime: 63.85\tLR: 1.0000\n",
            "Train Epoch: 47\tTotal_its: 0.66M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.3971\tTime: 64.22\tLR: 1.0000\n",
            "Train Epoch: 47\tTotal_its: 0.66M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 4.5558\tTime: 64.59\tLR: 1.0000\n",
            "Validation Loss: 4.3621\tPerp: 78.4252\n",
            "Train Epoch: 48\tTotal_its: 0.67M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0530\tTime: 65.14\tLR: 1.0000\n",
            "Train Epoch: 48\tTotal_its: 0.67M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 3.2328\tTime: 65.53\tLR: 1.0000\n",
            "Train Epoch: 48\tTotal_its: 0.68M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 3.8450\tTime: 66.04\tLR: 1.0000\n",
            "Validation Loss: 4.3391\tPerp: 76.6391\n",
            "Train Epoch: 49\tTotal_its: 0.68M [0.00M/0.01M]\tPercdone: 0.00\tLoss: 4.0384\tTime: 66.58\tLR: 1.0000\n",
            "Train Epoch: 49\tTotal_its: 0.69M [0.01M/0.01M]\tPercdone: 0.36\tLoss: 9.1302\tTime: 66.95\tLR: 1.0000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 49\tTotal_its: 0.69M [0.01M/0.01M]\tPercdone: 0.72\tLoss: 5.2233\tTime: 67.30\tLR: 1.0000\n",
            "Validation Loss: 4.7790\tPerp: 118.9911\n",
            "The end! 67.84 minutes\n"
          ]
        }
      ],
      "source": [
        "GCNNnet.model_fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62NlCmF95jTc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkr-as6n5jTc"
      },
      "outputs": [],
      "source": [
        "epoch_list,loss_list,perp_list,time_list=zip(*GCNNnet.modelvals['val_loss_list'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x0PSFzf5jTd",
        "outputId": "ce1ede91-1ff2-4b9b-d63e-28a79da12e32"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAESCAYAAADXMlMiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPW9+P/XzCQz2TNJyDKBYFhDJLIGBVmqAQQUCFIViksrirUWSlVouVJJC/rzgrQWr7T5VSj3qhSKCyCL4BJRkUVWSdi3hCWTkH0j68z5/jFkSsgyEyaZmSTv5+ORR5I5Z+a8z4ch7/nsKkVRFIQQQojbpHZ1AEIIIdo2SSRCCCEcIolECCGEQySRCCGEcIgkEiGEEA6RRCKEEMIhkkiEEEI4RBKJEEIIh0giEUII4RBJJEIIIRwiiUQIIYRDJJEIIYRwiCQSIYQQDvFwdQCtraCgDLO5YyxwHBLiR15eqavDcCkpAykDkDJw5P7VahVBQb7Nek67TyRms9JhEgnQoe61MVIGUgYgZeDM+5emLSGEEA6RRCKEEMIhkkiEEEI4RBKJEEIIh0giEUII4RBJJEIIIRwiiaQBq7ae4NPdF10dhhBCtAntfh7J7biaU0ZpebWrwxBCiDZBaiQN8NJqqKiscXUYQgjRJkgiaYCXVkNFlcnVYQghRJsgiaQBXjoPSSRCCGEnSSQNsNRIpGlLCCHsIYmkAdK0JYQQ9pNE0gAvrQdVNWZMZrOrQxFCCLcniaQBXloNAJVSKxFCCJskkTSgNpFI85YQQtjm9ETyzjvvEBMTw5kzZwCIiYlh0qRJJCYmkpiYyOnTp63npqSkMH78eMaOHctvf/tbysvLnRKjl9YyT7NcEokQQtjk1Jntx48f5+jRo0RGRtZ5fP369fj61t3asaysjFdffZW1a9cSHR3NwoULWb16NbNnz271OP9TI5GRW0IIYYvTaiRVVVUsXryYpKQkVCqVzfO//fZb4uLiiI6OBmD69Ol89tlnrRylhTRtCSGE/ZxWI1mxYgWTJ08mKiqq3rEnn3wSk8nEqFGjmDNnDlqtFqPRWKfmEhkZidFodEqstU1bFZWSSIQQwhanJJIjR46QmprKvHnz6h3btWsXBoOB0tJS5s+fz8qVK3nxxRdb7NohIX7Nfk71jRqT1suD0FD/FovFGdpavK1BykDKAKQMnHn/TkkkBw4c4MKFC4wePRqArKwsnnnmGd544w1GjBgBgJ+fH48++ihr1qwBwGAwsH//futrZGZmYjAYmn3tvLxSzGalWc+5XlYFwLXcMnJySpp9TVcJDfVvU/G2BikDKQOQMnDk/tVqVbM/gDulj+S5555j9+7dpKSkkJKSQkREBKtXr+auu+6ioqICgJqaGnbu3ElsbCwAI0eOJDU1lfT0dMDSIT9hwgRnhCud7UII0Qwu3Y/kwoULLFq0CJVKRU1NDQMHDmTu3LmApYayePFifvnLX2I2m4mNjWXhwoVOiUvroUalks52IYSwh0sSSUpKivXnLVu2NHremDFjGDNmjDNCqkOlUuGllRWAhRDCHjKzvRGyArAQQthHEkkjZAVgIYSwjySSRkjTlhBC2EcSSSOkaUsIIewjiaQR0rQlhBD2kUTSCC+thyyRIoQQdpBE0ggvnTRtCSGEPSSRNEKatoQQwj6SSBrhpfXAZFaorpF924UQoimSSBoh620JIYR9JJE0Qja3EkII+0giaYR37eZWkkiEEKJJkkgaIU1bQghhH0kkjfCSGokQQthFEkkjpI9ECCHsI4mkEdZEUilNW0II0RSnJ5J33nmHmJgYzpw5A8DRo0eZPHky48aNY+bMmeTl5VnPbepYa/PSSdOWEELYw6mJ5Pjx4xw9epTIyEgAFEVh/vz5LFq0iJ07dxIfH8/y5cttHnMG6WwXQgj7OC2RVFVVsXjxYpKSklCpVACkpqai0+mIj48HYPr06ezYscPmMWfw0Kjx0KikRiKEEDY4LZGsWLGCyZMnExUVZX3MaDRaaycAwcHBmM1mCgsLmzzmLLK5lRBC2ObhjIscOXKE1NRU5s2b54zL1RES4nfbz/Xx9kRRqwgN9W/BiFpXW4q1tUgZSBmAlIEz798pieTAgQNcuHCB0aNHA5CVlcUzzzzDk08+SWZmpvW8/Px8VCoVer0eg8HQ6LHmyMsrxWxWbiturUZFUXEFOTklt/V8ZwsN9W8zsbYWKQMpA5AycOT+1WpVsz+AO6Vp67nnnmP37t2kpKSQkpJCREQEq1ev5tlnn6WiooKDBw8CsH79eiZMmABAXFxco8ecRZq2hBDCNqfUSBqjVqtZtmwZSUlJVFZW0rlzZ958802bx5zFS6uhrKLaqdcUQoi2xiWJJCUlxfrzoEGD2LJlS4PnNXXMGby0GvKKK1x2fSGEaAtkZnsTpGlLCCFsk0TSBMt2uzIhUQghmiKJpAleOsu+7Ypye6O+hBCiI5BE0gQvrQeKAlXVsm+7EEI0RhJJE2S9LSGEsE0SSRNkTxIhhLBNEkkTZJdEIYSwTRJJE6RpSwghbJNE0oTaGkm51EiEEKJRkkiaIDUSIYSwTRJJE6SzXQghbJNE0gRrZ3ulJBIhhGiMJJImSNOWEELYJomkCWq1Cq2nWpq2hBCiCZJIbJAVgIUQommSSGyQFYCFEKJpTtvY6oUXXuDKlSuo1Wp8fHx49dVXiY2NJSEhAa1Wi06nA2DevHmMHDkSgKNHj7Jo0aI6OySGhIQ4K2SgNpFIjUQIIRrjtESydOlS/P39Afjyyy955ZVX2LhxIwBvv/02vXv3rnO+oijMnz+fN954g/j4eP72t7+xfPly3njjDWeFDEjTlhBC2OK0pq3aJAJQWlqKSqVq8vzU1FR0Oh3x8fEATJ8+nR07drRqjA2Rpi0hhGiaU/dsX7hwId9//z2KorBq1Srr4/PmzUNRFAYPHsxLL71EQEAARqORyMhI6znBwcGYzWYKCwvR6/V2XzMkxM+hmPX+XuQUVRAa6m/7ZDfQVuJsTVIGUgYgZeDM+3dqInn99dcB2LRpE8uWLePdd99l7dq1GAwGqqqqeP3111m8eDHLly9vsWvm5ZViNjuww6GiUFZeTU5OSYvF1FpCQ/3bRJytScpAygCkDBy5f7Va1ewP4C4ZtTVlyhT2799PQUEBBoMBAK1Wy4wZMzh8+DAABoOBzMxM63Py8/NRqVTNqo20BGnaEkKIpjklkZSVlWE0Gq2/p6SkEBgYiE6no6TEkjUVRWH79u3ExsYCEBcXR0VFBQcPHgRg/fr1TJgwwRnh1uGl1VBVbXasViOEEO2YU5q2ysvLmTt3LuXl5ajVagIDA0lOTiYvL485c+ZgMpkwm8306NGDpKQkANRqNcuWLSMpKanO8F9nu3lzKx8vp7YECiFEm+CUv4ydOnViw4YNDR7btGlTo88bNGgQW7Zsaa2w7OKl+896W5JIhBCiPpnZboMsJS+EEE2TRGKD7NsuhBBNk0Rig7csJS+EEE2SRGKD1EiEEKJpdieS2bNn8+WXX1JdXd2a8bgd2dxKCCGaZnciGTRoECtXrmTEiBEkJSVZJw62d9LZLoQQTbM7kcycOZONGzfywQcfEBAQwMsvv8zYsWN55513uHTpUmvG6FLStCWEEE1rdh9Jr169ePnll3nzzTfx9vZm5cqVPPzww/ziF7/g1KlTrRGjS2k91ahU0rQlhBCNadYMuwsXLvDpp5+ydetWPD09SUxMJDExkeDgYP71r3/xwgsvkJKS0lqxuoRKpbKst1UpNRIhhGiI3Ylk6tSpXL16lQcffJA///nP9O/fv87xp59+mvfff7/FA3QHsrmVEEI0zu5E8txzz1m3xW1Me6uN1JIVgIUQonF295EkJyc3mESmTp3aogG5I9m3XQghGmd3ImloZJaiKFy5cqVFA3JH0rQlhBCNs9m09bvf/Q6Aqqoq68+1rl69Ss+ePVsnMjfipdVQcr3K1WEIIYRbsplIunbt2uDPYJmkOH78+JaPys1I05YQQjTOZiKZPXs2AP3792fkyJG3faEXXniBK1euoFar8fHx4dVXXyU2NpaLFy+yYMECCgsL0ev1LF26lOjoaIAmjzmTNG0JIUTjmkwkBw4cYMiQIZYTPTzYu3dvg+cNGzbM5oWWLl2Kv78/AF9++SWvvPIKGzduJCkpiRkzZpCYmMjmzZtZtGgR7733HkCTx5xJRm0JIUTjmkwkf/rTn9i6dSsACxcubPAclUrFV199ZfNCtUkEoLS0FJVKRV5eHidOnGDNmjUATJw4kSVLlpCfn4+iKI0eCw4Otu/uWoiXVkONSaHGZMZDIwsmCyHEzZpMJLVJBFpmjsjChQv5/vvvURSFVatWYTQaCQ8PR6OxLIyo0WgICwvDaDSiKEqjx5yfSP6z3paftyQSIYS4md0TEi9evEi3bt3qPX7o0CEGDx5s12u8/vrrgGWf9mXLljF37lx7L3/bQkL8HH6N0BBfAHz8vAgN9nH49VpTaKi/7ZPaOSkDKQOQMnDm/dudSB577DFefPFFZsyYAUB1dTV//etf2bhxI3v27GnWRadMmcKiRYuIiIggOzsbk8mERqPBZDJx7do1DAYDiqI0eqw58vJKMZuVZj3nVtU3+kcyjUWoTe7b6R4a6k9OTomrw3ApKQMpA5AycOT+1WpVsz+A291O8/7777N+/Xqee+459u7dy09/+lNOnz7Npk2bbD63rKwMo9Fo/T0lJYXAwEBCQkKIjY21NqFt3bqV2NhYgoODmzzmbLIniRBCNM7uGkmfPn348MMPeeSRR5g5cyZTp061NlXZUl5ezty5cykvL0etVhMYGEhycjIqlYo//vGPLFiwgL/97W8EBASwdOlS6/OaOuZMskuiEEI0zu5Ekp2dze9//3s8PT1ZuHAh77zzDiEhIfzmN7/Bw6Ppl+nUqRMbNmxo8FiPHj348MMPm33MmWRzKyGEaJzdTVuJiYkMGDCADRs28MQTT7B582bS0tL46U9/2prxuQXdjRpJudRIhBCiHrtrJH//+98ZOHCg9ffw8HD++c9/umSCoLPVNm1VSo1ECCHqsbtGUptEjEYjR48etT7+1FNPtXxUbsZbOtuFEKJRdieSzMxMpk+fzoQJE3j66acB2LFjR6Mz3tsTD40ajVoliUQIIRpgdyJZtGgR9913H4cPH7Z2rg8fPrzZc0jaIuu+7dJHIoQQ9didSFJTU3nuuedQq9WoVCrAsn5WSUnHmPQjS8kLIUTD7E4kISEhZGRk1Hns3LlzzZ5p3lbJUvJCCNEwuxPJzJkzef755/n444+pqalh69atvPjii8yaNas143Mb0rQlhBANs3v47yOPPIJer+ff//43BoOBjRs3MnfuXMaMGdOa8bkNL62GcqmRCCFEPXYnEoAxY8Z0mMRxKy+tBwWlsm+7EELcqslE8tFHH9n1Io888kiLBOPOpGlLCCEa1mQi2bx5s80XUKlUHSSReFBRKU1bQghxqyYTyfvvv++sONyel84y/FdRFOvwZyGEEM3sIykuLmbXrl1cu3aNsLAw7rvvPgICAlorNrfipdVgVhSqa8xoPTWuDkcIIdyG3cN/9+7dS0JCAu+//z6pqal88MEHJCQksHfv3taMz23IUvJCCNEwu2skS5YsYfHixTz44IPWxz777DP+9Kc/sWPHjlYJzp3cvLlVgK/WxdEIIYT7sLtGcu3aNcaNG1fnsbFjx5Kbm2vzuQUFBcyaNYtx48YxadIkZs+eTX5+PgAxMTFMmjSJxMREEhMTOX36tPV5KSkpjB8/nrFjx/Lb3/6W8vJye8NtcbLdrhBCNMzuRDJlyhTWrl1b57F169YxZcoUm89VqVQ8++yz7Ny5ky1bthAVFcXy5cutx9evX8/mzZvZvHkzMTExgGWf91dffZXk5GS++OILfH19Wb16tb3htjhp2hJCiIbZ3bR1/Phx1q1bx6pVqwgPDyc7O5v8/Hz69evH448/bj3v1mQDoNfrueeee6y/DxgwgHXr1jV5vW+//Za4uDiio6MBmD59OgsWLGD27Nn2htyiZN92IYRomN2J5LHHHuOxxx5z+IJms5l169aRkJBgfezJJ5/EZDIxatQo5syZg1arxWg0EhkZaT0nMjISo9Ho8PVvlzRtCSFEw+xKJCaTiUuXLvGrX/0KrdaxjuYlS5bg4+PDE088AcCuXbswGAyUlpYyf/58Vq5cyYsvvujQNW4WEuLXIq+jeFgSiYfWk9BQ/xZ5zdbgzrE5i5SBlAFIGTjz/u1KJBqNhn/961/MmTPHoYstXbqUjIwMkpOTUast3TO1y9D7+fnx6KOPsmbNGuvj+/fvtz43MzPztpasz8srxWxWHIob4HpFNQC5eaXk5LjnHiyhof5uG5uzSBlIGYCUgSP3r1armv0BvFmd7bb6NZry1ltvkZaWxsqVK621mqKiIioqKgCoqalh586dxMbGAjBy5EhSU1NJT08HLB3yEyZMuO3rO0qatoQQomF295EcO3aMDz74gNWrVxMREVFnmZCGOthvdvbsWZKTk4mOjmb69OkAdOnShWeffZZFixahUqmoqalh4MCBzJ07F7DUUBYvXswvf/lLzGYzsbGxLt0fXqNW4+mhlkQihBC3cEpne69everMD7nZli1bGn2euy1bLysACyFEfXYnkocffrg142gTZN92IYSoz+4+EkVR2LBhA0899RSTJk0C4MCBA2zfvr3VgnM3sm+7EELUZ3ciWbFiBR999BHTpk2zzueIiIhg1apVrRacu5GmLSGEqM/uRLJx40aSk5N56KGHrB3tXbp04fLly60WnLvx0nrIvu1CCHELuxOJyWTC19cXwJpIysrK8PHxaZ3I3JD0kQghRH12J5JRo0bxxhtvUFVVBVj6TFasWMH999/fasG5G2naEkKI+uxOJK+88go5OTkMHjyYkpISBg4cSGZmJvPmzWvN+NyKdLYLIUR9Nof/lpeX8/e//50zZ85w5513snDhQnJzczEYDISGhjojRrfhpdVQWWXCrCioZd92IYQA7EgkixcvJi0tjZEjR/L5559TVFTEq6++6ozY3I6XzrJMSmWVCW9ds7a7F0KIdstm09Z3333H6tWr+d3vfse7777L119/7Yy43JJsbiWE81RVm3hvxymKSitdHYqwwWYiuX79OmFhYQDW5d47Kv2NvdrTjcUujkSI9i89q4RdRzNJvZDv6lCEDTbbZ0wmE/v27UNRLEux19TU1PkdYNiwYa0XoRu5q0cIoXovPv0+nQG9OtVZuFII0bLySywrgxeVSY3E3dlMJCEhIbzyyivW3/V6fZ3fVSoVX331VetE52Y8NGom3hvNmu2nOHo2l4G9O9ZgAyGcqaDYkkCKSqtcHImwxWYiSUlJcUYcbca9cRFs25PB5t0XpVYiRCvKv5FICsskkbg7u+eRCAuNWs2k4dFculbKkbO5rg5HiHbL2rQlne1uTxLJbRjaN5zwIG82776IWXF8G18hRH35JdK01VY4JZEUFBQwa9Ysxo0bx6RJk5g9ezb5+ZaRGEePHmXy5MmMGzeOmTNnkpeXZ31eU8dcqbZWcvlaKUfO5Lg6HCHapYKS2qatyjqDe4T7cUoiUalUPPvss+zcuZMtW7YQFRXF8uXLURSF+fPns2jRInbu3El8fDzLly8HaPKYO7jnznDCg32kViJEK6iuMVNcVoW3zoOqarPM3XJzTkkker2ee+65x/r7gAEDyMzMJDU1FZ1OR3x8PADTp09nx44dAE0ecwcatZrJw6O5klPG4dNSKxGiJRXc6BeJjvAHoFD6Sdya09f5MJvNrFu3joSEBIxGI5GRkdZjwcHBmM1mCgsLmzym1+vtvl5IiF+Lxn+zh0b58dn+S2zbl8G44d1Rq10/gis01N/VIbiclEHbL4PsGyO24np24mRGASoPj2bfU1svA0c58/6dnkiWLFmCj48PTzzxBF988UWrXy8vrxSzufWanh4c2pV/fHqCz3af5+7Y8Fa7jj1CQ/3JySlxaQyuJmXQPsrgwuUCAMICvABIv1pARKDO7ue3hzJwhCP3r1armv0B3KmjtpYuXUpGRgZ//etfUavVGAwGMjMzrcfz8/NRqVTo9fomj7mTu/uEYwjx4dPv01s1YQnRkeQXW4b+djNYPlXLyC335rRE8tZbb5GWlsbKlSvRai1rVsXFxVFRUcHBgwcBWL9+PRMmTLB5zJ2o1SomD+9GZm4Ze49nteq1FEVhw9fnOHO5sFWvI4SrFZRU4uvlQZC/Dg+NWhKJm3NK09bZs2dJTk4mOjqa6dOnA5b93leuXMmyZctISkqisrKSzp078+abbwKgVqsbPeZuhvQJY+cPl1iz/RQl16sZd3dUq8x4T88qYcf+S+QWltM7yr1qZkK0pPziSoL8vSytEH5aCmW9LbfmlETSq1cvTp8+3eCxQYMGsWXLlmYfcydqtYr5PxvImu0n2fD1Oc5nFjHzwdgW37Nkb5qlxnPqUqFsriXatfySCoIDLH0igX5aqZG4OZnZ3kK8dR78akocj93fkyNnclnyfwe5mlvWYq9fYzKz/2Q2XloNpeXVZOa03GsL4W4KSioJ9rckEr2vTob/ujlJJC1IpVIx/p6uzP/ZAK5XVPPa/x3kh5PZLfLaxy/mU3K9modHdQfg5KWCFnldIdxNdY2JkuvVBN0YsSU1EvcniaQVxHQNIunpu4kK8yN583HWf3XW4SUe9h7Pws/bk/sHdqZToBenMiSRiPapdmmU2hpJoJ+O65U1VFXL7HZ3JYmklQT56/jdjIHcP6gznx+4zPeptz+i63pFDUfO5jIkNgwPjZo+XYM4c7lQlmYR7VLt8vH/adqyjPIskuXk3ZYkklbkoVHz+Nje9OwSyL9TzlJ8m/8RDp2+RnWNmXv7RgDQ5w49ZRU1XM7uuNsei/ardvn4/zRtWRKKNG+5L0kkrUytUvHz8X2oqDKxPuXsbb3G3uNZhAd50z0yAIA+XYMAOCX9JKIdqm3aCqqtkfhZaiTS4e6+JJE4QedOvjw07A72Hc8m7ULzlsLPK6rg1KVChvWNsM5NCQ7wIizIW/pJRLuUX1yJn7cnOk8NcFONRJq23JYkEid5aNgdRAT78N7O01Q2o9Nw3wlL38rQuIg6j/fpGsSZK4WYzOYWjVMIV8svrrDWRgD8fTxRq1RSI3FjkkicxNNDw8/Hx5BbVMGnuy/a9RxFUdiTlkXPLoGE6b3rHOtzh57yShOXpJ9EtDM3zyEBS/NwgK+n9JG4MUkkThTTNYhR/Q3s/OEyl7Jtr8yZkV2CMe+6tZP9ZtZ+EmneEu1MfkklwTc62msF+ulkmRQ3JonEyR69vyd+Pp7872enbK4WvCctCw+NiiGxYfWO6f10GEJ8ZGKiaFeqqk2UllfXadoCyxBgqZG4L0kkTubr5cmMMb1Izyrhq0NXGj3PZDbzw4ls+vfohK+XZ4Pn9OkaxNnLRdSYpJ9EtA/WyYgBdROJZXa71EjclSQSFxjSJ4x+PUL45NsLXCssb/Cc4xcLKL5ezbC4+s1atfrcEURltYn0rI67gY9oX2r3IQnyv6Vpy1dHyfVqGVzipiSRuIBKpeKJB3oDsPAf+/ifj49ZJx3W2ns8C18vD/r1CGn0dWK6WpaSl34S0V7kN1Ij0ftpUYDismoXRCVscfpWu8KiU6A3i34RzzdHM9l/IpsjZ3Px0XkwJDaM+JgwjpzJ4d67DHhoGs/1AT5aOof6cupSARPvjXZe8EK0kvxb1tmqVTuXpLC0sl7/iXA9SSQuZAjxZfroXjx6fw9Ophew53gWe49n8c1RyxbDDY3WulWfrkF892NmndqMEG1VQXEFft6eeHpo6jweeGN2u3S4uyenJZKlS5eyc+dOrl69ypYtW+jd29K0k5CQgFarRaezfMqYN28eI0eOBODo0aMsWrSozg6JISGNN/W0VRq1mrjuIcR1D6GiqobDZ3IoKq2iR+cAm8/t0zWIrw5d4aKxmEhDoBOiFaL1WIb+1q9x6H1v1EhkCLBbclofyejRo1m7di2dO3eud+ztt99m8+bNbN682ZpEFEVh/vz5LFq0iJ07dxIfH8/y5cudFa7LeGk9uDfOwIShd9i1XW9MVz0qpJ9EtA/5xZUE39LRDlIjcXdOSyTx8fEYDAa7z09NTUWn0xEfHw/A9OnT2bFjR2uF12b5eXsSFe4nCziKdqGgpIKgBmokHho1ft6eMgTYTblFH8m8efNQFIXBgwfz0ksvERAQgNFoJDIy0npOcHAwZrOZwsJC9Hq93a8dEuLXGiG7lUF9wtn2/UWqqk2Ehvq7OhyXa04ZVFTWoFar0HpqbJ/chrTF90FFVQ1lFTVERQQ0GH9IoBfl1Wa7760tlkFLcub9uzyRrF27FoPBQFVVFa+//jqLFy9u0SasvLxSmzPI27quob5U15g5lZGPIbB+s0BHEhrqT06O/fNqXn/vIHo/Hb+eelcrRuVczS0Dd2HMKwNAp1Y1GL+flwfX8svsure2WgYtxZH7V6tVzf4A7vJ5JLXNXVqtlhkzZnD48GHr45mZmdbz8vPzUalUzaqNdBS9u+hRqeDYuVxXh9KmXMkp5XxmMT+ez6OySrZxdbXGZrXXCvTTUSh9JG7JpYnk+vXrlJRYsqaiKGzfvp3Y2FgA4uLiqKio4ODBgwCsX7+eCRMmuCxWd+bj5UF0hD+pkkiaZW+aZYn+GpOZkzJYweVqt9gNCmi4Vh3op6W4rEq2mHZDTmvaeu211/j888/Jzc3l6aefRq/Xk5yczJw5czCZTJjNZnr06EFSUhIAarWaZcuWkZSUVGf4r2hY327BbN2TwaqtJ5g6qnu91VNFXWZFYd+JbPp2C+bc1SKOnc9lQK9Org6rQ7NusevXcI1E76vDZFYoLa8mwEfrzNCEDU5LJH/4wx/4wx/+UO/xTZs2NfqcQYMGsWXLltYMq914aGg0Op0nm789z8FT13jg7q48OLQrXlqXd4O5pdMZBRSUVDItoSc6Tw0/ns9DURS7hlyL1lFQUkmAjyeeHg03lNw8BFgSiXtxeR+JaBk6rYZfTOzL/zdrKAN6dWLrnnQW/P/7+PbHzDY52MCSmgcTAAAZBklEQVRsVlq1CWPP8Sy8dRoG9OxEvx4hFJRUciWnrNWuJ2zLL66st1jjzfTWLXdlCLC7kUTSznTSe/N8YhwLnxpMmN6b//3sFElrfuDM5UJXh9Ys/9hynFdX7W+V7VUrq00cOp3D4JgwtJ4a68KYP0ofk0vll1Q02tEOMinRnUkiaad6RAbyX08M4oUpcVRWmVi69jD/TjlLdY3zRyeVV9ZYR+TY40pOKT+cvIYx7zpvrjtCcVnL/uE4ejaXiioTw26sZab303FHuD/Hzue16HVE8xQ0Mqu9lnWZFJmU6HYkkbRjKpWK+D5hLH7mbu4b2JmdP1zmj2sOcNFY7LQYqmvM/PfawyT98wfKK2vses6O/ZfQeqr59cN3kVdUwfL1Rygtb7nlw/cezyI4QGddhh+gX48QzmcWteh1hP0qqmq4XlnT4Kz2WjqtBi+tRmokbkgSSQfgpfXgyXExvDStPxVVJl5/7xAbv73glJ0VP/3+IpevlVJaXs2O/Zdsnp9XVMH+E9mM6h/J4JhQ5jzSj6z8cpavP0JZheN/5IvLqki7kM/QOyNQ39Sx3q9nCIoCqRekVuIKBY0sH38ry97tkkjcjSSSDiSuWwhLnrmboX3D2bInndf+7yAnMwq4mltGdsF18osrKCqr4npFdYs0gZ27UsT2fRmM6GdgSJ8wPj9wmSIbfwS+OHgZRYEHhkQB0Dc6mNlT7yIzt4y//PtHu2s1jdl/MhuzojCsb3idx7sZAvD38ZTmLRepnUNia9i6Ze92adpyNzI2tIPx8fLk2Yl3Mqh3KO/tOMWb6440em6/HiFMGdmN6Ajby9nfqrLKxKptJwj29+Jno3tRVFbFodM5bN2TzuNjezf4nNLyar45msk9d4bRKdC7Thy/mhLH3zam8daGH3lpWv/bHta8Ny2LruF+dA6tuwSEWqXiru4h/HguF5PZjEYtn7Gc6T9b7NqqkWhJN3bcpU/clSSSDmpQ71B6R+k5e7mQapOZGpOZGpNi/V5cVsU3R6+y+H8PMrBXJxJHdKNruP2LwG3YdY6cgnJ+N2Mg3joPvHUejOxvYNeRqzwwJIpQvXe953x95CqV1SYm3HNHvWMDe4Xyy8l9Sd58nBUfHuPXU+/Cz9uzWfdszCsjPauE6Qk9Gzzev2cn9qRlcf5qMb2jZCkeZ6pt2rKVSPR+OgrLcmXOj5uRRNKB+Xl7MrB3aKPHHxp2B18cuMzOA5c5suYA8TGhJI7oVu/T/K3SLubx9WFLwojpGmR9fPLwbuxJy2LTdxeYNalvnedUVZv48uBl7uoeQpewhl8/vk8Yz5rMrNp6kgXJe5k8PJqEwV2a3I74ZnuPZ6FSwd13hjd4vG90MBq1imPn8ySROFl+SQUBvlqb/5aBflqqqs1UVJnw1smfr4aYzQq5heVOvabU30WjvHUeTB7RjWW/GsbEe6NJvZjPotU/kLw5rdGRX2UV1azZfgpDiA9TR3WvcyzIX8eY+C7sO57N5WuldY59n2qk5Ho1Dw7t2mRMQ/tG8MeZQ+gWGcD6lHO8umo/R87koNiYvGhWFPamZdM3Otg6se1WPl4e9OoSyLHzMp/E2SwbWtnei12GANu2fV8Gc5Z/7dRrSiIRNvl6eTJ1VHeWPT+M8UO7cux8Hkv+7yBvfHCIQ6dz6sycX/vFGYrLqpg16c4G9/h4cOgdeOs8+OSb89bHzGaFHT9contkgF01gS6hfrw8bQC/fbQ/arWK//kklTfXHSEjq/G283NXisgrrmBYXESTr92vRyeu5JSRV1TR4HFFUdi2N509aUabcQr7FZRU2rU+nExKtO2Hk9e4w9D8fk1HSN1Q2M3fR8uj9/Vk4rBovvsxky8OXmHlxlRC9V6MiY/CR+fBvuPZTBnReAe9r5cnE4Z25eNvLnDmciG9o/QcPH2NnMIKHru/Z7Pavfv1CKFvtyC+OZrJpu8usvh/DzA4NpxuEX706qInOsLf2lSyJy0LnaeGQb0ab8qrfc0NX5/j2IU87h9Yd1toRVH415dn+erQFcDyx++hYdF2xysal19SQZ87gmyeF+gne7c3JaewnCs5pTwwtH4/Y2uSRCKazVvnwQN3d2V0fBeOnMll54FLrPvyLADREf48OKzpN/GY+Ci+PHSFj745z389PojP9l8iPNiHgTb+yDdEo1aTMKgLQ+8MZ9u+DH48l8fBk9kAeHqo6WYIoFeXQA6eusag3qHotE3vhGgI8aFToBfHzuXWSSSKovDvlHN8degKY+OjKL5excffXKCiysTUUd2l49cB5ZU1lFeamlwepZZeaiRNOnrW0ix7T1wEOHG5fUkk4rZp1Gri+4QR3yeM81eL2H8imzHxtju/dZ4aEod3472dp1n/1Tkyskr4xYQ+qNW3/8fYx8uTR+/ryQuPDuRceh7nrhRy9koRZ68UsWP/JUxmhRF3Nd2sBZbVAPr36MR3xzKpqjah9dSgKAoffXOezw9cZvTgLkwf3RNFsdzHtr0ZlFfWMGNs7zoTHIX98q2TEW03bfnoPPDQqCWRNOLI2Rw6d/IlspOfU3eIlEQiWkSPzoH06Bxo9/kj+hnY8cMlvjh4mUBfbb0Jgo4I9NUyOCaMwTFhgGVOS35JBYYQX7ue369nCF8dvsKpS4X06xHCxu8u8tm+S9w3sDMzxvRCpVKhUsHPx8fgrdOw84fLVFSZePrBPjL/5DYU2DmHBCyJXu+nlaatBpSWV3PmchETbAxYaQ1OedcvXbqUhIQEYmJiOHPmjPXxixcvMm3aNMaNG8e0adNIT0+365ho+zw0auuorrFDovD0aLrJyRE6rcbuJALQp6seraeaY+dz+XT3RbbuSWdUfwNPPNC7ThOWSqXisft7MmWkZVhz8qbjVNe0/rIz7U2+jS12bxXop5UaSQOOnc/FrCi31UTsKKfUSEaPHs1TTz3F448/XufxpKQkZsyYQWJiIps3b2bRokW89957No+J9mFInzB8vDzo09V2J6szeXpouPOOYL790UiNyczwuAieGt+nwaYrlUrF5OHd8NJ6sP6rs7y5/gg9IgPw0KhvfKmsP9/VPZiwIB8X3JF7yy+uQAWNDsu+ld5XR2ae7B1zqyNncwn00xJtsH/icEtxSiKJj4+v91heXh4nTpxgzZo1AEycOJElS5aQn5+PoiiNHgsODnZGyMIJVCoVcd1CXB1Gg/r3DOHouVyG9g3n6QdjbfZ/PDAkCm+dho+/ucCl7BJMJgXTLRuKees0/Coxjrju7nnPrlBjMnMhs5gAP9uTEWsF+mk5mVHQypG1LdU1JtIu5DMsLsIlfXUu6yMxGo2Eh4ej0ViaNDQaDWFhYRiNRhRFafSYJBLhDCP6GdD76YjrHmz3IICR/SIZ2S/S+rtZUTDdWHKmsLSS5M3HeevDH5kxpjejB3dprdAbdCI9n1OXCogM8SUq3B9DsI9DgxtaQkFJJX/fnMa5K0Ukjuhm9/MC/XRcr6yxDoYQcCK9gMpqEwN7dXLJ9dt9Z3tISNPLebQ3oaHOr9a6m5Yqg4hw+wcP2NIV+HO3Tvx57SHWfnGGwuvVzEqMQ2Pnp/Dmqi2DzNxS/vnpcfYfz6pzXOupoZshgG43BkkM7x+JvxP3Qf/xTA7L1x6ioqqGeY8P5ieD7E+sUTfmKGl0noQ20ffVkf4vnNp13rKe3eD/9Dc68/5dlkgMBgPZ2dmYTCY0Gg0mk4lr165hMBhQFKXRY82Vl1faJvcsvx2hof5OHfLnjty9DGY9FEuQn5Zt318kPbOIXyX2xcereYtP2hIa6s+lKwVs2ZPOFwcu4+Gh5qc/6c7owV3IKazgUnYJl7JLuXythG8OX2HH3nQ++OwkPx/fhwGt/InWrChs25vBpu8uEBHsw7zpA4js5NusfzP1jfkRFy8VoDE3PLjB3d8HLcmsKOxNNRLXLZjCguuAY/evVqua/QHcZYkkJCSE2NhYtm7dSmJiIlu3biU2NtbadNXUMSHaKrXaMtLLEOzDeztP8/r7h/jNI/3w8/akuKyKkuvVN75bfvbx8sDQyRdDsA9B/jqbEx/NZoWd+zJ4b9txiq9XM+IuA1N/0t3akR0V5kdUmB/D77KcrygK6VklrNl+irc/PsbwuAh+NqZXiyc3sAxPfXfLCVIv5DH0znCeGh9zW9sB1E5KlPW2LC5kFlNcVuWyZi0AlWJrtbsW8Nprr/H555+Tm5tLUFAQer2ebdu2cf78eRYsWEBxcTEBAQEsXbqU7t0tQ0KbOtYcUiPpWNpSGZzKKGDlxlTKKuzbrEun1WAI9sEQ4ktwgI7KKhPlVTVUVJmoqKyhvMpEYWkl+cWV9OwcyM/G9KKbnWsu1ZjMbPk+nW17Mwjw9eQXE2Lp18PxQQGVVSZOXy4g7UI+B05fo6y8mp+N7sV9Azvf9moARWVVvPg/u3l8bP2+pqpqExnZJQy5qzOFBR1jZNeHu87x+Q+XWfGbEdYPAM6ukTglkbiSJJKOpa2VwbWC6+w9no23VoO/r5YAHy0BvloCfDzx9faktLwaY951jHlldb4XllbipfXAW6fBW+uBl1aD1419X+4bHEVsl4Db+kOdnlXM6q0nuZpbxsh+BqYl9MLHy75ag1lRqKkxk5V/neMX80m7mM/ZK4XUmBQ8PdTEdNXz8Mjudie3pq7z3LJdTBjalZ/+pAdgqe18ffgKXx66Qsn1arp3DuQX42PoYmPLg6ZkZJWw8bsLmBWFUf0iGdCrk90jy5xp4bv70PvpmP+zgdbHJJG0MEkkHUtHKYOmNnZytAyqa8xs3n2Rz/ZnAOCpUaO5aU6MRq1Co1FjMpmprrF8VdVYNke7WZdQX/p2CyauWwi9owJbdNLpS+/sJq5bCJOHR/P5gct8eyyTqmoz/XqEENctmG37Migrr2bqqB48MCSqWSPU8osr+OTbC+xNy8LX2xOtp5r84koCfLWMuMvAqP4Gt5kPZMwrY+G7++vVzjpMH4kQ4va15iKRnh5qHrmvB4NjQjlyNvfGrplmTDftoGnZjliFp4cGrYcaz5u+9H467owOtmvJk9sV6Kfj0Jkc9qRZNisbemc44+7paq2BTBjRg7+sPciGr89x9FwuzzwU2+CunDcrr6zhs/2X+PyHS5gVGD+0Kw8NjcZLqyHtYh7fHM1kx/5LbN+XwZ3RQQyPMxAcoMPHyxNvnQYfnSdeOk2z53FU15jJL6lA76dD18zhzLWLNA7o6br+EZBEIoRoRDdDgMPNUK0lKtSP7PzrPDAkijHxXertZaL31zF76l3sScviX1+eYdE/f+Bno3sxsp9l5Gd1jZnyyhqu3/i6lFXC5u/TKS6r4u7YMB75SQ863ZR4+vXoRL8enSgoqWT3sUy+/TGTd7eeqBeXCvDSeeDv7UmAr5ZAX22d7zqthryiCnIKy61f+cWVKICvlwcJg7owenAXAnztG4p95GwuXcP9CAm0veBla5KmrXakozTrNEXKoGOUgclsxmy21J4acnMZ5BaV889tJzl1qRAfnQeV1aZ6qw4A9OwcyLTRPekRaXv+kNmscPlaKWUV1VyvqPlPUqqwfC8tr6aotJLi65bvtw6oCPDVEqr3IkzvTajem+AAL348l8vRs7l4eKgZHhfBuLu7Eh7ceBNaUVkVL/3PbiaP6FZvQqc0bQkhhA0atRp7+707BXoz72cD+fbHTC5nl+Kt87jRFOVx42cPAny1REf4291kqFaruCPC/gl/NSYzxWVVVFRZ9l1paNjzqP6RGPPK2PnDZXanGvnmaCaDeocyenAXuoT54eddd0j2j+dyUcClw35rSSIRQrR7apWK+wZ0tn1iK/HQqO3aStgQ4ssvJvTh4ZHd+PLQFb4+fJVDZ3IAy14soUHehOm9CQvy5vjFfEICvIgKc/3qHZJIhBDCzQT66fjpT3rw4NA7OJlRwLUCS3/KtcJyMrJKOHwmB5NZYfw9Xd1id05JJEII4aa8dR4M6l1/fxGT2UxhSRWBfs5bH60pkkiEEKKN0ajVLh+pdTP3m6YphBCiTZFEIoQQwiGSSIQQQjhEEokQQgiHSCIRQgjhEEkkQgghHNLuh/82Z/no9qCj3W9DpAykDEDK4Hbv/3ae1+4XbRRCCNG6pGlLCCGEQySRCCGEcIgkEiGEEA6RRCKEEMIhkkiEEEI4RBKJEEIIh0giEUII4RBJJEIIIRwiiUQIIYRDJJG0QUuXLiUhIYGYmBjOnDljffzixYtMmzaNcePGMW3aNNLT010XZCsrKChg1qxZjBs3jkmTJjF79mzy8/MBOHr0KJMnT2bcuHHMnDmTvLw8F0fbel544QUmT57MlClTmDFjBidPngQ61nsB4J133qnz/6EjvQcSEhIYP348iYmJJCYm8t133wFOLgNFtDkHDhxQMjMzlfvvv185ffq09fEnn3xS2bRpk6IoirJp0yblySefdFWIra6goEDZt2+f9ff//u//Vv7rv/5LMZvNypgxY5QDBw4oiqIoK1euVBYsWOCqMFtdcXGx9ecvvvhCmTJliqIoHeu9kJaWpjzzzDPKfffdp5w+fbrDvQdu/TugKIrTy0BqJG1QfHw8BoOhzmN5eXmcOHGCiRMnAjBx4kROnDhh/ZTe3uj1eu655x7r7wMGDCAzM5PU1FR0Oh3x8fEATJ8+nR07drgqzFbn7+9v/bm0tBSVStWh3gtVVVUsXryYpKQkVCrLYoMd7T3QEGeXQbtf/bejMBqNhIeHo9FoANBoNISFhWE0GgkODnZxdK3LbDazbt06EhISMBqNREZGWo8FBwdjNpspLCxEr9e7MMrWs3DhQr7//nsURWHVqlUd6r2wYsUKJk+eTFRUlPWxjvgemDdvHoqiMHjwYF566SWnl4HUSESbt2TJEnx8fHjiiSdcHYpLvP766+zatYsXX3yRZcuWuTocpzly5AipqanMmDHD1aG41Nq1a/n000/5+OOPURSFxYsXOz0GSSTthMFgIDs7G5PJBIDJZOLatWv1msDam6VLl5KRkcFf//pX1Go1BoOBzMxM6/H8/HxUKlW7/SR6sylTprB//34iIiI6xHvhwIEDXLhwgdGjR5OQkEBWVhbPPPMMGRkZHeo9UPvvqtVqmTFjBocPH3b6/wNJJO1ESEgIsbGxbN26FYCtW7cSGxvb7poybvbWW2+RlpbGypUr0Wq1AMTFxVFRUcHBgwcBWL9+PRMmTHBlmK2mrKwMo9Fo/T0lJYXAwMAO81547rnn2L17NykpKaSkpBAREcHq1at59tlnO8x74Pr165SUlACgKArbt28nNjbW6f8PZGOrNui1117j888/Jzc3l6CgIPR6Pdu2beP8+fMsWLCA4uJiAgICWLp0Kd27d3d1uK3i7NmzTJw4kejoaLy8vADo0qULK1eu5PDhwyQlJVFZWUnnzp1588036dSpk4sjbnm5ubm88MILlJeXo1arCQwM5Pe//z19+/btUO+FWgkJCSQnJ9O7d+8O8x64fPkyc+bMwWQyYTab6dGjB3/4wx8ICwtzahlIIhFCCOEQadoSQgjhEEkkQgghHCKJRAghhEMkkQghhHCIJBIhhBAOkUQihJuLiYkhIyPD1WEI0ShZa0uIZkpISCA3N9e6lhXAww8/zKJFi1wYlRCuI4lEiNuQnJzMvffe6+owhHAL0rQlRAv55JNPmD59OkuWLGHw4MGMHz+evXv3Wo9nZ2fz/PPPc/fddzN27Fg2bNhgPWYymUhOTmbMmDEMHDiQqVOn1ln+ZM+ePTzwwAMMGTKEP/3pT8g8YuFOpEYiRAs6duwY48ePZ9++fXzxxRfMnj2br776Cr1ez8svv0zPnj357rvvuHDhAk8//TRRUVEMGzaMNWvWsG3bNv7xj3/QrVs3Tp8+bV36BWDXrl189NFHlJaWMnXqVO6//35GjRrlwjsV4j+kRiLEbfj1r39NfHy89au2dhEcHMzPf/5zPD09efDBB+nWrRu7du3CaDRy6NAh5s2bh06nIzY2lkcffZTNmzcD8OGHHzJ37ly6d++OSqWiT58+BAUFWa83a9YsAgICiIyM5J577uHUqVMuuW8hGiI1EiFuw8qVK+v1kXzyySeEh4dbd+oDiIyM5Nq1a1y7do3AwED8/PzqHEtLSwMgKyuLrl27Nnq90NBQ68/e3t6UlZW11K0I4TCpkQjRgrKzs+v0XxiNRsLCwggLC6OoqIjS0tI6x8LDwwGIiIjg0qVLTo9XiJYgiUSIFpSfn897771HdXU1n332GefPn+cnP/kJBoOBgQMH8pe//IXKykpOnTrFRx99xKRJkwB49NFHWbFiBenp6SiKwqlTpygoKHDx3QhhH2naEuI2PP/883Xmkdx7772MHj2afv36kZGRwdChQ+nUqRNvv/22ta/jL3/5C0lJSYwcOZKAgADmzJnD8OHDAXj66aepqqpi5syZFBQU0L17d1auXOmSexOiuWQ/EiFayCeffMKHH37IunXrXB2KEE4lTVtCCCEcIolECCGEQ6RpSwghhEOkRiKEEMIhkkiEEEI4RBKJEEIIh0giEUII4RBJJEIIIRwiiUQIIYRD/h+roLRXvedABgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(list(range(3,51)),perp_list[2:])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C7k1UvU5jTd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}